PCA creates new [] that maximise [] between data-points 
-----PCA creates new dimensions that maximise variance between data-points 


Given covariance matrix $S$ and principal component $u_1$, what is the projected variance?
-----$J = u_1^T S u_1$ 


In PCA, to maximise projection variance, we wish to choose the [] of $S$ (the covariance matrix) with the $M$ [] []
-----In PCA, to maximise projection variance, we wish to choose the eigenvectors of $S$ (the covariance matrix) with the $M$ largest eigenvalues


PCA Steps (high level)
-----1) Standardise data 2) Compute covariance matrix 3) Calculate eigenvectors and eigenvalues 4) Select feature vector 5) Project data onto principal components


What to Scree Plots show?
-----How variance is distributed across the Principle Components 


What can a Scree Plot be used for?
-----Subjectively finding an "elbow" in the data which enables us to choose number of PCs


Difference between Linear Discriminant Analysis and PCA?
-----PCA maximises variance of all the data while LDA maximises separability of known classes 


Is LCA supervised or unsupervised? Why?
-----Supervised - we provide class information, to "tell" the system where discrimination should occur


What are the two high-level objectives of LDA?
-----1) Maximise distance between projected group means ($m$) 2) Minimise scatter within each group ($s$)


Fisher's Linear Discriminant
-----$J(\textbf{w}) = |\tilde{\textbf{m}}_1 - \tilde{\textbf{m}}_2|^2 / (\tilde{s}_1^2 - \tilde{s}_2^2)$


LDA Steps 
-----1) Standardise the data 
2) Calculate means (class and overall) 
3) Compute within-class scatter matrix 
4) Compute between-class scatter matrix 
5) Combine scatter matrices 
6) Compute eigenvectors and eigenvalues 
7) Sort eigenvalues 
8) Create feature vector 
9) Project data


K Means steps 
-----1) Randomly initialise $k$ centroids 
2) Assign each datapoint to the nearest centroid 
3) Calculate the means of the datapoints in each cluster 
4) Update the centroids to be the means calculated in 3) 
5) Repeat from step 2 to 4 until values converge 


What is the elbow method to determine the number of clusters to use in K-Means?
-----Measure the sum of squared distances from data points to associated centroids - this value decreases as $k$ increases. Plotting this allows us to find an "elbow" which can help us choose a value of $k$


Is the Agglomerative Algorithm a top-down or bottom-up hierarchical clustering approach?
-----Bottom-up; each observation starts in its own cluster


Is Divisive Clustering a top-down or bottom-up hierarchical clustering approach?
-----Top-down; all observations start in one cluster, that cluster is then split recursively


How do you read a Dendrogram?
-----Dendrograms are representations of hierarchies - horizontal links are connections, Y-Axis is Distance between connections, Height of connections is important - the lower the branch the smaller the distance.


Problems with decision trees
-----Highly dependent on training data - tend to overfit and not generalise well 


What is ensemble learning?
-----Using a combination of weak classifiers to create a stronger overall classifier 


Why do we use ensemble learning with decision trees?
-----Reduces the effect of one classifier (decision tree) overfitting the data 


What is a bagging in decision trees?
-----Bagging (Bootstrap Aggregation) is used to reduce the variance of a decision tree. 
Several subsets of data are created by sampling the dataset randomly with replacement. 
Each subset is used to train different decision trees, which are aggregated in the final model.


What is the random forest technqiue?
-----A random forest is an ensemble model of multiple decision trees, where each tree is trained on a different randomly sampled "bag" of data. 
Feature bagging is also used which means that each candidate split in the learning process uses a random subset of the features. 


What is out-of-bag error?
-----The proportion of all out-of-bag samples that were misclassified


Why is AdaBoost "adaptive"?
-----Subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers


How can boosting be conceptualised as a regression problem?
-----Boosting is a form of linear regression in which the features of each sample $x_{i}$ are the outputs of some weak learner $h$ applied to $x_{i}$.


What is dynamic time warping?
-----DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restrictions and rules.