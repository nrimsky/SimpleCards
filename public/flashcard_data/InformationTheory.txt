Entropy in terms of $p$
-----$E(-\log_{2}(p(x)))$


Second moment of distribution
-----$E(x^2) = \sigma^2 + \mu^2$


The Shannon information content of an outcome is
-----$\log_{2}(1/p_i) = -\log_{2}(p_i)$


Entropy in terms of sum
-----$-\sum_{x \in X} p(x) \log_{2}(p(x))$


1 nat
-----1 nat = $\log_{2}(e)$ bits = 1.44 bits


Joint Entropy
-----$H(x,y) = E[-\log_{2}p(x,y)]$ = $-\sum_{x,y}(p(x,y)\log_{2}p(x,y))$


Conditional Entropy
-----$H(Y|X) = E[-\log_{2}P(y|x)]$ $= \sum_{x \in X} p(x)H(Y|X = x)$


Average Row Entropy
-----Take the weighted avg of each row using $p(x)$ as a weight


$H(X_({1:n})$
-----$H(X_({1:n})$ $= \sum_{1 \rightarrow i} H(x_i | x_{1:i-1})$


Conditional Mutual Information
-----$I(x;y|z) =$ $H(x|z) + H(y|z) - H(x,y|z)$


Chain rule for mutual information
-----$I(x_{1:n} ; y) =$ $\sum_{1 \rightarrow i} I(x_i; y | x_{1:i-1})$


$|X|$
-----The cardinality of a set $X$


Jensen's Inequality
-----if $h''(x) >(=) 0$, then $E(h(x)) >(=) h(E(x))$


Chain rule for 2 variables
-----$H(x,y) = H(y|x) + H(x)$


Mutual Information Definition
-----a measure of the extent to which variables are related


Mutual Information
-----$I(x;y) = H(x) - H(x|y) \leq H(x)$


In [], mutual information is the amount of [] transmitted through a [] []
-----In communications, mutual information is the amount of information transmitted through a noisy channel


Relative Entropy definition
-----A measure of how different two probability mass vectors are


Why is $D(p||q)$ not a true distance?
----- It is asymmetric between p and q; It does not satisfy the triangle inequality


Relative Entropy formula
-----$D(p||q) = \sum_{x \in X} p(x) log [p(x)/q(x)]$


Information (Gibbs') Inequality
-----$D(p||q) \geq 0$


Information Inequality Corollaries
----- Uniform distribution has the highest entropy
- Mutual Information is non-negative
- Conditioning reduces entropy
- Independence bound


What is the independence bound
-----$H(X_{1:n}) = \sum_{1 \rightarrow n}H(X_i | x_{1:i-1})$ $\leq \sum_{1 \rightarrow n}H(X_i)$


Entropy of s\rightarrowchastic process
-----$H({x_i}) = H(x_1) + H(x_2|x_1) +$ ...


Entropy rate
-----$H(X) = lim_{n \rightarrow \infty}1/n H(x_{1:n})$ if limit exists


Entropy rate estimates the [] [] per [] []
-----Entropy rate estimates the additional entropy per new sample


S\rightarrowchastic process {x_i} is stationary iff
-----$p(x_{1:n} = a_{1:n}) = p(x_k+1:n = a_{1:n})$ $\forall k, n, a_i \in X$


If a Markov process is [] and [] then it has [] [] [] distribution $p_s$
-----If a Markov process is irreducible and aperiodic then it has exactly one stationary distribution $p_s$


Irreducible
-----You can go from any state $a \rightarrow any b$ in a finite number of steps


Aperiodic
-----$\forall$ states $a$ the possible times to go from $a \rightarrow a$ have highest common factor 1


$p_s$ is the eigenvector of [] with $\lambda$ = []
-----$p_s$ is the eigenvector of $T^T$ with $\lambda = 1$


Entropy of stationary process
-----$H(x) = lim_{n \rightarrow \infty}H(x_n | x_{1:n}-1)$


Entropy of stationary Markov process
-----$H(x) = H(x_n | x_n-1)$ $= \sum_{i,j} -p_{Si} t_{ij} \log (t_{ij})$


Source coding theorem
-----n i.i.d rv's each with entropy $H(X)$ can be compressed into more than $nH(X)$ bits as $n \rightarrow \infty$


Source code
----- $C$ is a mapping $X \rightarrow D+$ 
- $X$ is a rv of the message 
- $D+$ is the set of all final length strings from $D$ 
- $D$ is often binary 


$C+$
-----$C+$ is a mapping $S+ \rightarrow D+$ formed by concatenating $C(x_i)$ without punctuation


Non singular
-----$x_1 != x_2 \rightarrow C(x_1) != C(x_2)$


Instantaneous / prefix code
-----No codeword is a prefix of another; can be decoded instantaneously without reference to future codewords


How is a code tree used to create an instantaneous code?
-----Form a D-ary tree where D = |D|. Each codeword is a leaf. Each node along the path to a leaf is a prefix of the leaf. Some leaves may be unused.


Kraft Inequality
-----Limit of codeword lengths of instantaneous codes. For codeword lengths $l1, l2, ..., l|X|$ $\rightarrow$ $\sum_{i = 1 \rightarrow |X|} D^{-l_i} \leq 1$


McMillan Inequality
-----If uniquely decodable $C$ has codeword lengths $l1, l2, ..., l|X|$ then $\sum_{i = 1 \rightarrow |X|} D^$-l_i$ \leq 1$


Implication of McMillan Inequality
-----Uniquely decodable codes doesn't offer further reduction of codeword lengths than instantaneous codes


What is the simplified optimisation problem for finding the optimal codes?
-----We simplify by considering non-integer $l_i$. Minimise $\sum_{1 \rightarrow |X|} p(x_i)l_i$ subject to $\sum_{1 \rightarrow |X|} D^$-l_i$ = 1$


Bounds on Optimal Code Length
-----$log_D(p(x_i)) \leq l_i \leq -log_D(p(x_i)) + 1$


Average shortest length
-----$H_D(x) \leq L^* < H_D(x) + 1$


Source coding theorem inequality of symbol by symbol encoding
-----$H_D(x) \leq E[l(x)] \leq H_D(x) + 1$


Source coding theorem inequality of block encoding
-----$n^{-1} E[l(x_{1:n})] \rightarrow H_D(X)$


An optimal binary instantaneous code must satisfy $p(x_i) >$ ...
-----$p(x_i) > p(x_j) \Rightarrow l_i \leq l_j$


Why do the two longest codewords of an optimal binary instantaneous code have the same length?
-----We are using a tree code (no codeword is a prefix of another) - can chop of a bit off the longer codeword if one is longer.


In Huffman coding we encode higher probability symbols with a [] code
-----In Huffman coding we encode higher probability symbols with a shorter code


Huffman coding uses a [] code table for encoding a string where the [] code table has been derived in a particular way based on the estimated [] [] [] for each possible [] of the [] in the string. 
O([]) to read file and compute character frequencies. 
O([]) to create optimal code. 
-----Huffman coding uses a variable-length code table for encoding a string where the variable-length code table has been derived in a particular way based on the estimated probability of occurrence for each possible value of the character in the string. 
O(N) to read file and compute character frequencies. 
O(C log C) to create optimal code. 


Shannon-Fano codes are suboptimal \in the sense that they do not always achieve the [] possible expected codeword [], as [] coding does. However, Shannon-Fano codes have an expected codeword length within [] [] of optimal. Fano's method usually produces encoding with [] expected lengths than Shannon's method.
-----Shannon-Fano codes are suboptimal \in the sense that they do not always achieve the lowest possible expected codeword length, as Huffman coding does. However, Shannon-Fano codes have an expected codeword length within 1 bit of optimal. Fano's method usually produces encoding with shorter expected lengths than Shannon's method.


Issues with Huffman Coding
-----Requires the probability distributions of the source but for many practical applications the underlying probability is unknown


Universal encoding does not depend on the [] of the []
-----Universal encoding does not depend on the distribution of the source


Lempel-Ziv coding is [] []: it achieves the [] [] for any [] [] source
-----Lempel-Ziv coding is asymptoically optimum: it achieves the entropy rate for any stationary ergodic source


Run Length Coding (RLE)
-----Runs of data are stored using number of occurrence + symbol pairs


Lempel Ziv coding works best on data with [] patterns, so the initial parts of a message see [] []. As the message grows, however, the compression ratio tends [] to the [].
-----Lempel Ziv coding works best on data with repeated patterns, so the initial parts of a message see little compression. As the message grows, however, the compression ratio tends asymptotically to the maximum.


With a Markov chain $x \to y \to z$, if you know y then [] z gives you no [] [] about x
-----With a Markov chain $x \to y \to z$, if you know y then observing z gives you no additional information about x


With a Markov chain $x \to y \to z$, if you know y then [] x gives you no [] [] about z
-----With a Markov chain $x \to y \to z$, if you know y then observing x gives you no additional information about z


With a Markov chain $x \to y \to z$, $I(x;z|y) = [] \leftrightarrow H([]) = H([])$
-----With a Markov chain $x \to y \to z$, $I(x;z|y) = 0 \leftrightarrow H(z|y) = H(z|x,y)$


Recovering [] from a [] is a good model for machine learning / data science task were we want to estimate the [] of something given an [] effect ($x \to y \to z$ where z in an estimate of x)
-----Recovering data from a channel is a good model for machine learning / data science task were we want to estimate the value of something given an observable effect ($x \to y \to z$ where z in an estimate of x)


Markov chain property is [] $(x \to y \to z \leftrightarrow z \to y \to x)$
-----Markov chain property is symmetrical $(x \to y \to z \leftrightarrow z \to y \to x)$


If $x \to y \to z$, $I([]) \geq I([])$ and $I([]) \geq I([])$
-----If $x \to y \to z$, $I(x;y) \geq I(x;z)$ and $I(x;y) \geq I(x;y | z)$


Data processing theorem in words
-----Given a Markov chain $x \to y \to z$, processing y cannot add new information about x, and knowing z does not increase the amount y tells you about x


Meaning of "sufficient statistic" given Markov chain $x \to y \to z$
-----z contains all the information in y about x; preserves mutual information $I(x;y) = I(x;z)$


In information theory, Fano's inequality relates the [] information lost in a [] [] to the [] of the categorisation error. It is used to find a [] [] on the error probability of any decoder as well as the [] [] for minimax risks in density estimation.
-----In information theory, Fano's inequality relates the average information lost in a noisy channel to the probability of the categorisation error. It is used to find a lower bound on the error probability of any decoder as well as the lower bounds for minimax risks in density estimation.


Let the random variables X and Y represent input and output messages with a joint probability $P(x,y)$. Let $e$ represent an occurrence of error; i.e., that $X != X'$, with $X' = f(Y)$ being an approximate version of $X$. Fano's inequality is []
-----Let the random variables X and Y represent input and output messages with a joint probability $P(x,y)$. Let e represent an occurrence of error; i.e., that $X != X'$, with $X' = f(Y)$ being an approximate version of $X$. Fano's inequality is
$H(X|Y) \leq H(P_e)+P(e)\log ( |X| - 1 )$


How to use Fano's inequality to compute the lower bound on the probability of error $p_e$
-----$p_e \geq \dfrac{H(x|y) - H(p_e)}{\log (|X| - 1)}$


Weaker version of Fano's inequality that does not depend on $p_e$ which can be used to get probability of error
-----$p_e \geq \dfrac{H(x|y) - 1}{\log (|X| - 1)}$


Asymptotic Equipartition Principle (AEP)
-----Although there are many series of results that may be produced by a random process, the one actually produced is most probably from a loosely defined set of outcomes that all have approximately the same chance of being the one actually realised. $\dfrac{-1}{n \log p(x)}$ convergences in probability to $H(x)$.


Typicality
-----A measure of how well a category member represents that category (in Information Theory this means how message symbol frequency matches source symbol frequency).


Convergence in probability
-----The probability of an "unusual" outcome becomes smaller and smaller as the sequence progresses. An estimator is called consistent if it converges in probability to the quantity being estimated. Convergence in probability is also the type of convergence established by the weak law of large numbers.


A sequence ${X_n}$ of random variables converges in probability towards the random variable $X$ if $\forall \epsilon > 0$ if:
-----$\lim_{n \rightarrow \infty}Pr(|X_n - X| > \epsilon) = 0$