Entropy in terms of expected value
-----$H(X) = E[-\log(P(X))]$


Entropy in terms of sum
-----$H(X) = -\sum_{i=1}^n P(x_i)\log P(x_i)$


Shannon information content of an outcome $E$
-----$I(E) = -\log_{2}(p(E))$


$\sum_{n=1}^{\infty}nr^n =$
-----$r/(1-r)^2$


$\log_a(x)' =$
-----$\log_a(e)/x = 1/ (x \ln(a))$


Maximum value of $H(P)$ where $P$ is an $n$-dimensional probability vector
-----$\log_2(n)$, which occurs when all elements of $H(P)$ are equal (uniform distribution).


Minimum value of $H(P)$ where $P$ is an $n$-dimensional probability vector
-----0, which occurs when only one element of $H(P)$ is non-0. There are $n$ possible vectors of this kind.


Average number of yes - no questions need to find $x$ is in the range...
-----$[H(X), H(X) + 1)$


Joint Entropy
-----$H(X, Y) = E[-\log P(X,Y)]$ = $-\sum_{i, j} p(x_i,y_j) \log_{2}p(x_i, y_j)$


Conditional Entropy
-----$H(Y|X) = E[-\log_{2}P(Y|X)]$ $= -\sum_{i, j}p(y_i, x_j)\log p(y_i | x_j)$


Uniform bound on entropy
-----$H(X) \leq \log(|X|)


Chain rule for 2 variables proof
-----$H(X,Y) = E[-\log(P(X,Y))] = $ $E[-\log(P(X)P(Y|X))] = $ $E[-\log(P(X)) -\log(P(Y|X))] = $ $E[-\log(P(X))] + E[-\log(P(Y|X))] = $ $H(X) + H(Y|X)$


Mutual Information is a measure of...
-----Mutual Information is a measure of the extent to which variables are related


Mutual Information
-----$I(X;Y) = H(X) - H(X|Y) \leq H(X)$


$H(X_{1:n})$
-----$H(X_{1:n})$ $= \sum_{i = 1}^{n} H(X_i | X_{1:i-1})$


$H(X_{1:n})$ given $X_i$ is i.i.d.
-----$H(X_{1:n}) = nH(X_i)$


Conditional Mutual Information
-----$I(X;Y|Z) =$ $H(X|Z) + H(Y|Z) - H(X, Y|Z)$


Chain rule for mutual information
-----$I(X_{1:n} ; Y) =$ $\sum_{i = 1}^{n} I(X_i; Y | X_{1:i-1})$


$|X|$
-----The cardinality of a set $X$


Jensen's Inequality
-----$h''(x) >(=) 0 \Rightarrow E[h(x)] >(=) h(E[x])$


Relative Entropy definition
-----A measure of how different two probability mass vectors are


Why is $D(P||Q)$ not a true distance?
-----It is asymmetric between P and Q; It does not satisfy the triangle inequality


Relative Entropy formula
-----$D(P||Q) = \sum_{x \in \mathcal{X}} P(x) \log [P(x)/Q(x)]$


Information (Gibbs') Inequality
-----$D(P||Q) \geq 0$


Information Inequality Corollaries
-----Uniform distribution has the highest entropy; Mutual Information is non-negative; Conditioning reduces entropy; Independence bound


What is the independence bound
-----$H(X_{1:n}) = \sum_{i = 1}^{n} H(X_i | X_{1:i-1})$ $\leq \sum_{i = 1}^{n} H(X_i)$


Entropy of Stochastic process
-----$H(X_i) = H(X_1) + H(X_2|X_1) +$ ...


Entropy rate
-----$H(X) = lim_{n \rightarrow \infty} H(X_{1:n})/n$ if limit exists


Entropy rate estimates the [] [] per [] []
-----Entropy rate estimates the additional entropy per new sample


If a Markov process is [] and [] then it has [] [] [] distribution $p_s$
-----If a Markov process is irreducible and aperiodic then it has exactly one stationary distribution $p_s$


Irreducible
-----You can go from any state $a$ to any $b$ in a finite number of steps


Aperiodic
-----$\forall$ states $a$ the possible times to go from $a \rightarrow a$ have highest common factor 1


$p_s$ is the eigenvector of [] with $\lambda$ = []
-----$p_s$ is the eigenvector of $T^T$ with $\lambda = 1$


A stochastic process is stationary if...
-----$P[X_1=x_1, X_2=x_2, ..., X_n = x_n]$ $= P[X_{1+k}=x_1, X_{2+k}=x_2, ..., X_{n+k} = x_n]$ for every $n$, shift $k$ and $x_n \in \mathcal{X}$.


A Markov chain is time invariant if...
-----$P[X_{n+1} = b|X_n = a] = P[X_2 = b|X_1 = a]$ $\forall a, b \in \mathcal{X}$


Entropy of stationary process
-----$H(X) = lim_{n \rightarrow \infty}H(X_n | X_{1:n-1})$


For a stationary time-invariant Markov process. the entropy rate is given by...
-----$H(X) = H(X_2|X_1)$


Source coding theorem
-----$n$ i.i.d rv's each with entropy $H(X)$ can be compressed into more than $nH(X)$ bits as $n \rightarrow \infty$


Source code
-----$C$ is a mapping $X \rightarrow D+$, $X$ is a rv of the message, $D+$ is the set of all final length strings from $D$, $D$ is often binary 


$C+$
-----$C+$ is a mapping $S+ \rightarrow D+$ formed by concatenating $C(X_i)$ without punctuation


Non singular
-----$X_1 \neq X_2 \rightarrow C(X_1) \neq C(X_2)$


Instantaneous / prefix code
-----No codeword is a prefix of another; can be decoded instantaneously without reference to future codewords


Instantaneous code $\rightarrow$ [] [] code $\rightarrow$ [] code 
-----Instantaneous code $\rightarrow$ Uniquely decodable code $\rightarrow$ Non-singular code 


Uniquely decodable code definition
-----A term usually applied to variable-length codes: ensures that codewords can be recognized unambiguously. Eg: symbol A can be distinguised from any other symbol B and any other arbitrary sequence CD.


Non-singular code definition
-----A code is non-singular if each source symbol is mapped to a different non-empty bit string, i.e. the mapping from source symbols to bit strings is injective. $X_1 \neq X_2 \rightarrow C(X_1) \neq C(X_2)$


How is a code tree used to create an instantaneous code?
-----Form a D-ary tree where D = |D|. Each codeword is a leaf. Each node along the path to a leaf is a prefix of the leaf. Some leaves may be unused.


Kraft Inequality
-----Limit of codeword lengths of instantaneous codes. For codeword lengths $l_1, l_2, ..., l_{|X|}$ the sum $\sum_{i = 1}^{|X|} D^{-l_i} \leq 1$


McMillan Inequality
-----If uniquely decodable $C$ has codeword lengths $l_1, l_2, ..., l_{|X|}$ then $\sum_{i = 1}^{|X|} D^{-l_i} \leq 1$


Implication of McMillan Inequality
-----Uniquely decodable codes doesn't offer further reduction of codeword lengths than instantaneous codes


What is the simplified optimisation problem for finding the optimal codes?
-----We simplify by considering non-integer $l_i$. Minimise $\sum_{i = 1}^{|X|} p(x_i)l_i$ subject to $\sum_{i = 1}^{|X|} D^{-l_i} = 1$


Bounds on Optimal Code Length
-----$-log_D(p(x_i)) \leq l_i \leq -log_D(p(x_i)) + 1$


Average shortest length
-----$H_D(X) \leq L^* < H_D(X) + 1$


Source coding theorem inequality of symbol by symbol encoding
-----$H_D(X) \leq E[l(X)] \leq H_D(X) + 1$


Source coding theorem inequality of block encoding
-----$n^{-1} E[l(X_{1:n})] \rightarrow H_D(X)$


An optimal binary instantaneous code must satisfy $p(x_i) >$ ...
-----$p(x_i) > p(x_j) \Rightarrow l_i \leq l_j$


Why do the two longest codewords of an optimal binary instantaneous code have the same length?
-----We are using a tree code (no codeword is a prefix of another) - can chop of a bit off the longer codeword if one is longer.


In Huffman coding we encode higher probability symbols with a [] code
-----In Huffman coding we encode higher probability symbols with a shorter code


Huffman coding uses a [] code table for encoding a string where the [] code table has been derived in a particular way based on the estimated [] [] [] for each possible [] of the [] in the string. 
O([]) to read file and compute character frequencies. 
O([]) to create optimal code. 
-----Huffman coding uses a variable-length code table for encoding a string where the variable-length code table has been derived in a particular way based on the estimated probability of occurrence for each possible value of the character in the string. 
O(N) to read file and compute character frequencies. 
O(C log C) to create optimal code. 


Shannon-Fano codes are suboptimal in the sense that they do not always achieve the [] possible expected codeword [], as [] coding does. However, Shannon-Fano codes have an expected codeword length within [] [] of optimal. Fano's method usually produces encoding with [] expected lengths than Shannon's method.
-----Shannon-Fano codes are suboptimal in the sense that they do not always achieve the lowest possible expected codeword length, as Huffman coding does. However, Shannon-Fano codes have an expected codeword length within 1 bit of optimal. Fano's method usually produces encoding with shorter expected lengths than Shannon's method.


Issues with Huffman Coding
-----Requires the probability distributions of the source but for many practical applications the underlying probability is unknown


Universal encoding does not depend on the [] of the []
-----Universal encoding does not depend on the distribution of the source


Lempel-Ziv coding is [] []: it achieves the [] [] for any [] [] source
-----Lempel-Ziv coding is asymptoically optimum: it achieves the entropy rate for any stationary ergodic source


Run Length Coding (RLE)
-----Runs of data are stored using number of occurrence + symbol pairs


Lempel Ziv coding works best on data with [] patterns, so the initial parts of a message see [] []. As the message grows, however, the compression ratio tends [] to the [].
-----Lempel Ziv coding works best on data with repeated patterns, so the initial parts of a message see little compression. As the message grows, however, the compression ratio tends asymptotically to the maximum.


With a Markov chain $x \to y \to z$, if you know y then [] z gives you no [] [] about x
-----With a Markov chain $x \to y \to z$, if you know y then observing z gives you no additional information about x


With a Markov chain $x \to y \to z$, if you know y then [] x gives you no [] [] about z
-----With a Markov chain $x \to y \to z$, if you know y then observing x gives you no additional information about z


With a Markov chain $x \to y \to z$, $I(X;Z|Y) = [] \leftrightarrow H([]) = H([])$
-----With a Markov chain $x \to y \to z$, $I(X;Z|Y) = 0 \leftrightarrow H(Z|Y) = H(Z|X,Y)$


Recovering [] from a [] is a good model for machine learning / data science task were we want to estimate the [] of something given an [] effect ($x \to y \to z$ where z in an estimate of x)
-----Recovering data from a channel is a good model for machine learning / data science task were we want to estimate the value of something given an observable effect ($x \to y \to z$ where z in an estimate of x)


Markov chain property is [] $(x \to y \to z \leftrightarrow z \to y \to x)$
-----Markov chain property is symmetrical $(x \to y \to z \leftrightarrow z \to y \to x)$


If $x \to y \to z$, $I([]) \geq I([])$ and $I([]) \geq I([])$
-----If $x \to y \to z$, $I(X;Y) \geq I(X;Z)$ and $I(X;Y) \geq I(Z;Y | Z)$


Data processing theorem in words
-----Given a Markov chain $x \to y \to z$, processing y cannot add new information about x, and knowing z does not increase the amount y tells you about x


Meaning of "sufficient statistic" given Markov chain $x \to y \to z$
-----z contains all the information in y about x; preserves mutual information $I(X;Y) = I(X;Z)$


In information theory, Fano's inequality relates the [] information lost in a [] [] to the [] of the categorisation error. It is used to find a [] [] on the error probability of any decoder as well as the [] [] for minimax risks in density estimation.
-----In information theory, Fano's inequality relates the average information lost in a noisy channel to the probability of the categorisation error. It is used to find a lower bound on the error probability of any decoder as well as the lower bounds for minimax risks in density estimation.


Let the random variables X and Y represent input and output messages with a joint probability $P(X,Y)$. Let $e$ represent an occurrence of error; i.e., that $X \neq X'$, with $X' = f(Y)$ being an approximate version of $X$. Fano's inequality is []
-----Let the random variables X and Y represent input and output messages with a joint probability $P(X,Y)$. Let e represent an occurrence of error; i.e., that $X \neq X'$, with $X' = f(Y)$ being an approximate version of $X$. Fano's inequality is
$H(X|Y) \leq H(P_e)+p_e\log ( |X| - 1 )$


How to use Fano's inequality to compute the lower bound on the probability of error $p_e$
-----$p_e \geq (H(X|Y) - H(P_e))/(\log (|X| - 1))$


Weaker version of Fano's inequality that does not depend on $p_e$ which can be used to get probability of error
-----$p_e \geq (H(X|Y) - 1)/(\log (|X| - 1))$


Asymptotic Equipartition Principle (AEP)
-----Although there are many series of results that may be produced by a random process, the one actually produced is most probably from a loosely defined set of outcomes that all have approximately the same chance of being the one actually realised. $-1 /(n \log P(X)))$ convergences in probability to $H(X)$.


Typicality
-----A measure of how well a category member represents that category (in Information Theory this means how message symbol frequency matches source symbol frequency).


Convergence in probability
-----The probability of an "unusual" outcome becomes smaller and smaller as the sequence progresses. An estimator is called consistent if it converges in probability to the quantity being estimated. Convergence in probability is also the type of convergence established by the weak law of large numbers.


A sequence $\{X_n\}$ of random variables converges in probability towards the random variable $X$ if $\forall \epsilon > 0$ if:
-----$\lim_{n \rightarrow \infty}Pr(|X_n - X| > \epsilon) = 0$


Individual probability of an element of the typical set
-----$x \in T_{\epsilon}^{(n)} \Rightarrow \log{p(x)} = -nH(X) \pm n\epsilon$


Total probability in typical set
-----$p(x \in T_{\epsilon}^{(n)}) \geq 1 - \epsilon \forall{n > N_{\epsilon}}$


Size of typical set 
-----$(1 - \epsilon)2^{n(H(X) - \epsilon)} \leq |T_{\epsilon}^{(n)}| \leq 2^{n(H(X) - \epsilon)}$


Channel capacity definition in words
-----The highest rate in bits per channel use that can be transmitted reliably; the maximum mutual information


Source coding aim
-----Compresses the data to remove redundancy


Channel coding aim
-----Adds redundancy / structure to protect against channel errors (an error correction code)


Time-Invariant Transition-Probability matrix
-----$(Q_{Y | X})_{i, j} = p(Y = y_j | X = x_i)$ which has size $|X| \times |Y|$


Sum of rows of Transition-Probability matrix
-----1


Average column sum of Transition-Probability matrix
-----$|X||Y|^{-1}$


For a memoryless channel, $p(y_n | x_{1:n}, y_{1: n-1}) =$
-----$p(y_n | x_{1:n}, y_{1: n-1}) = p(y_n|x_n)$


Binary symmetric channel
-----$X = [0, 1], Y = [0, 1]$ $Q = \begin{pmatrix}1-f&f\\f&1-f\end{pmatrix}$ ($f$ is probability of error)


Binary erasure channel
-----$X = [0, 1], Y = [0, ?, 1]$ (? is erasure) $Q = \begin{pmatrix}1-f&f&0\\0&f&1-f\end{pmatrix}$ ($f$ is probability of error)


Z channel
-----$X = [0, 1], Y = [0, 1]$ $Q = \begin{pmatrix}1&0\\f&1-f\end{pmatrix}$ ($f$ is probability of error)


Difference between symmetric and weakly symmetric channels
-----Symmetric: rows are permutations of each other; columns are permutations of each other. Weakly Symmetric: rows are permutations of each other; columns have the same sum.


Weakly symmetric channel property consequences
-----Rows are permutations of each other $\rightarrow$ if x is uniform then y is uniform; Columns have the same sum $\rightarrow$ each row of Q has the same entropy which is the average row entropy 


Capacity of a DMC channel
-----$C = \text{max}_{P_X} I(X; Y)$


Capacity for $n$ uses of DMC channel 
-----$C^{(n)} = (1/n) \text{max}_{P_{X_{1:n}}} I(X_{1:n}; Y_{1:n})$


How to maximise $n$-use channel capacity
-----We can maximise $I(X;Y)$ by maximising each $I(X_i; Y_i)$ independently and taking $X_i$ to be iid.


Capacity of weakly symmetric channel
-----$C = \log{|Y|} - H(Q_1)$ where $Q_1$ is the row entropy


Capacity of binary symmetric channel
-----$1 - H(f)$


Capacity of binary erasure channel
-----$C = 1 - f$


For large $n$, an average input sequence $X_{1:n}$ corresponds to about [] typical output sequences.
-----For large $n$, an average input sequence $X_{1:n}$ corresponds to about $2^{nH(Y|X)}$ typical output sequences.


Maximum number of distinct sets of output sequences is [] so we can send [] bits per channel use
-----Maximum number of distinct sets of output sequences is $2^{n(H(Y) - H(Y|X))} = 2^{nI(Y;X)}$ so we can send $I(Y;X)$ bits per channel use


$\mathbf{x}, \mathbf{y}$ is the i.i.d sequence $\{ x_i, y_i \}$ for $1 \leq i \leq n$. Then the probability of a particular sequence $p(\mathbf{x}, \mathbf{y})$ is:
-----$p(\mathbf{x}, \mathbf{y}) = \prod_{i=1}^N p(x_i, y_i)$


Jointly Typical Set 
-----$J_\epsilon^{(n)} = \{ \mathbf{x}, \mathbf{y} \in XY^n : |-n^{-1} \log p(\mathbf{x}, \mathbf{y}) - H(x, y)| < \epsilon \}$


Size of Jointly Typical Set
-----$ (1 - \epsilon) 2^{n(H(x,y) - \epsilon)} < |J_\epsilon^{(n)}| \leq 2^{n(H(x,y) + \epsilon)}$


Bounds on $p(\mathbf{x}', \mathbf{y}' \in J_\epsilon^{(n)})$ if $p_{x'} = p_x$ and $p_{y'} = p_y$ with $x'$ and $y'$ independent:
-----$ (1 - \epsilon) 2^{-n(I(x;y) + 3\epsilon)}$ $\leq p(\mathbf{x}', \mathbf{y}' \in J_\epsilon^{(n)})$ $\leq 2^{-n(I(x;y) - 3\epsilon)}$


For a discrete memoryless channel with known $\mathbf{Q}_{y|x}$, an $(M, n)$ code is:
-----A fixed set of $M$ codewords $\mathbf{x}(w) \in X^n$ for $w = 1:M$, A deterministic decoder $g(\mathbf{y}) \in 1:M$


The rate of an an $(M, n)$ code
-----$R = (\log M)/n$ bits/transmission