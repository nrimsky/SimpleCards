A point $x \in F$ is a global minimiser iff
-----$f(x) \leq f(y) \forall y \in F$


A point $x \in F$ is a strict global minimiser iff
-----$f(x) < f(y) \forall y \in F$


Given $P_1$, a point $x \in F$ is a local minimiser if
-----There exists a $\rho > 0$ such that $f(x) \leq f(y) \forall y \in F$ and $||y-x|| < \rho$


Given $P_1$, a point $x \in F$ is a local minimiser if
-----There exists a $\rho > 0$ such that $f(x) < f(y) \forall y \in F$, $y \neq x$ and $||y-x|| < \rho$


Theorem: Let $f: R^n \rightarrow R$. Assume that $f$ is [] ($C^0$). Suppose that $F$ is []. Then there exists a [] [] for $f \in F$.
-----Theorem: Let $f: R^n \rightarrow R$. Assume that $f$ is continuous ($C^0$). Suppose that $F$ is compact. Then there exists a global minimiser for $f \in F$.


A set $F$ is compact if it is...
-----Closed and bounded


A set $F$ is closed if...
-----Its boundary is in the set


A set $F \in R^n$ is bounded if...
-----it is centred in a ball of some radius $R > 0$ centred at the origin


A function $f$ of the form $x^TQx + c^Tx$ with $Q > []$ is [] []
-----A function $f$ of the form $x^TQx + c^Tx$ with $Q > 0$ is radially unbounded


$C^1$
-----If vector $\nabla f$ is defined and continuous then we say $f \in C^1$


$C^2$
-----If $n \times n$ symmetric matrix $\nabla^2 f$ (Hessian of $f$) is defined and continuous then we say $f \in C^2$


Number of functions in the Hessian matrix of a function of $n$ variables
-----$n^2$


Number of functions in the gradient vector of a function of $n$ variables
-----$n$


Given continuous $f: R^n \rightarrow R$. A level set of $f$ is:
-----Any non empty set described by the relation $L(\alpha) = { x \in R^n: f(x) \leq \alpha }$ with $\alpha \in R$


Given continuous $f: R^n \rightarrow R$, if there exists a point $x_0$ such that $L(f(x_0)))$ is [], then the function $f$ has a [] [] contained in $L(f(x_0))$.
-----Given continuous $f: R^n \rightarrow R$, if there exists a point $x_0$ such that $L(f(x_0)))$ is compact, then the function $f$ has a global minimiser contained in $L(f(x_0))$.


Suppose we have $f: R^n \rightarrow R$, continuous. All level sets of $f$ are compact iff...
-----for any sequence $\{x_k\}$ we have the property $\lim_{k \rightarrow \infty} f(x_k) = +\infty$


Problem: min $f(x)$, $x \in R^n$, $f \in C$ has a solution if...
-----there exists $x_0$ such that $L(f(x_0))$ is compact .


Given a direction $d \in R^n$, $d$ is a descent direction for $f$ at $x^*$ if there exists $\delta > 0$ such that:
-----$f(x^*  + \lambda d) < f(x^*) \forall{\lambda \in (0, \delta)}$.


Why in theory would you need to naively test infinitely many points to determine if a direction is a descent direction?
-----We would be checking $f(x^*  + \lambda d) < f(x^*) \forall \lambda \in (0, \delta)$, where $(0, \delta)$ is a continuous set meaning it has infinitely many points.


Given continuous $f: R^n \rightarrow R$, $x^*$ and $d$. What are the conditions for $d$ to be a descent direction for $f$ at $x^*$?
-----$\nabla^T f(x^*) d \leq \nabla f(x^*)$, $d \neq 0$


Directional derivative of $f$ along $d$ at $x$
-----$\nabla^T f(x^*)d$


The gradient $\nabla f$ of a function $f$ at $x$ is [] to the [] of the [] [] $f(x) = 0$ of the function at $x$.
-----The gradient $\nabla f$ of a function $f$ at $x$ is perpendicular to the tangent of the level set $f(x) = 0$ of the function at $x$.


$<\nabla f(x^*), d>$ =
-----$|| \nabla f(x^*) || \cdot || d || \cos(\theta)$


Given continuous $f: R^n \rightarrow R$, what is the condition for the the point $x^*$ to be a local minimiser?
-----$\nabla f(x^*) = 0$


Not all points for which $\nabla f(x^*) = 0$ are [] [] of $f$, which means this is a [] but not [] condition
-----Not all points for which $\nabla f(x^*) = 0$ are local minimisers of $f$, which means this is a necessary but not sufficient condition


Local minimisers of $f$ belong to the set of points...
-----$\Omega = {x \in R^n :\nabla f(x) = 0 }$


$\Omega = {x \in R^n :\nabla f(x) = 0 }$ is called...
-----the set of stationary points of $f$


A saddle point is a [] in one [] and a [] in another []
-----A saddle point is a minimiser in one direction and a maximiser in another direction


Why is $\nabla f(x^*) = 0$ for all local minimisers? (proof by contradiction)
-----Given $x^*$ and suppose it is a local minimiser. Suppose $\nabla f(x^*) \neq 0$. Then I can find a descent direction for $f$ at $x^*$, for example $d = -\nabla f(x^*)$. This means that close to $x^*$ there are points $x'$ such that $f(x') < f(x^*) \Rightarrow x^* $ is not a minimum (contradiction).


How do we remove saddle points for the candidate optimal solutions?
-----Use second order derivative


Second order necessary condition for $f \in C^2$
-----$x^*$ is a local minimiser of f iff $\nabla f(x^*) = 0$ and $x^T \nabla^2 f(x^*) x \geq 0 \forall x \in R^n$


Taylor series expansion of $f(x^*  + \lambda x)$
-----$f(x^*) + \nabla^T f(x^*) x$ $+ (1/2) \lambda^2 x^T \nabla^2 f(x^*) x$ $+ \text{H.O.T} = f(x^*) + 0 +$ $(1/2) \lambda^2 x^T \nabla^2 f(x^*) x + \text{H.O.T}$


What is the quadratic form?
-----Given symmetric matrix M. We define the quadratic form to be $x^T M x$.


What holds for a symmetric matrix?
-----$M = M^T$


Any matrix $M'$ can be written as $M_s + M_a$ where $M_s =$ [] and $M_a =$ [] ($M_s$ is [] and $M_a$ is []). We can get $M_s$ using [] and $M_a$ using [].
-----Any matrix $M'$ can be written as $M_s + M_a$ where $M_s = M_s^T$ and $M_a = -M_a^T$ ($M_s$ is symmetric and $M_a$ is antisymmetric). We can get $M_s$ using $(M' + M'^T)/2$ and $M_a$ using $(M' - M'^T)/2$.


Positive semidefinite
-----$M \geq 0; x^T M x \geq 0 \forall x$


Positive definite
-----$M > 0; x^T M x > 0 \forall x \neq 0$


Negative definite
-----$M < 0; x^T M x < 0 \forall x$


Negative semidefinite
-----$M \leq 0; x^T M x \leq 0 \forall x$


The determinant of a positive definite matrix is always []
-----The determinant of a positive definite matrix is always positive


If $M$ is symmetric, all eigenvalues of $M$ are []
-----If $M$ is symmetric, all eigenvalues of $M$ are real


How do we know the sign of the eigenvalues given $M$?
-----We see if M is positive definite / semidefinite, negative definite / semidefinite or indefinite and the same fact applies to the eigenvalues. E.g. if positive definite, all eigenvalues > 0. For indefinite we have both positive and negative eigenvalues.


What tends to zero involving the Lagrange Remainder as $\lambda \rightarrow 0$?
-----$\text{H.O.T} / (\lambda^2 || x ||^2) \rightarrow 0$


$f: R^n \rightarrow R$ and $C^2$. $x^*$ is a strict [] minimiser of $f$ if $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) > 0$ (is strictly [] []).
-----$f: R^n \rightarrow R$ and $C^2$. $x^*$ is a strict local minimiser of $f$ if $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*) > 0$ (is strictly positive definite).


The condition $\nabla^2 f(x^*) > 0$ implies that [] for all $y$ sufficiently close to $x$.
-----The condition $\nabla^2 f(x^*) > 0$ implies that $\nabla^2 f(y) > 0 \forall y$ sufficiently close to $x$.


Lagrange Remainder
-----$1/2 (y - x^* )^T \nabla^2 f(\xi) (y - x^* )$


Why do we use the Taylor series expansion?
-----We know the value of a function at some point x and we want to know the value of the same function at another point y


The point $\xi$ in the Lagrange remainder expression $1/2 (y - x^* )^T \nabla^2 f(\xi) (y - x^* )$ is on a [] line between [] and [].
-----The point $\xi$ in the Lagrange remainder expression $1/2 (y - x^* )^T \nabla^2 f(\xi) (y - x^* )$ is on a straight line between $x^*$ and $y$.


Because of the sufficient condition of optimality, the point [] in the Taylor series expansion has to be [] point, and so the [] at that point is equal to 0.
-----Because of the sufficient condition of optimality, the point $x^*$ in the Taylor series expansion has to be stationary point, and so the gradient at that point is equal to 0.


Because of the continuity argument, if $y$ is sufficiently close to $x^*$ then [] in the [] [] $1/2 (y - x^* )^T \nabla^2 f(\xi) (y - x^* )$ will also be sufficiently close to $x^*$. This means that the quantity [] is also positive definite.
-----Because of the continuity argument, if $y$ is sufficiently close to $x^*$ then $\xi$ in the Lagrange remainder $1/2 (y - x^* )^T \nabla^2 f(\xi) (y - x^* )$ will also be sufficiently close to $x^*$. This means that the quantity $\nabla^2 f(\xi)$ is also positive definite.


Taylor series expansion of function given $x^*$ satisfies the sufficient conditions of optimality
-----$f(y) = f(x^*) + 1/2 (y - x^* )^T \nabla^2 f(\xi) (y - x^* )$


$f(y) - f(x^*)$
-----$1/2 (y - x^* )^T \nabla^2 f(\xi) (y - x^* )$ (Lagrange remainder of Taylor series expansion)


$f(y) - f(x^*)$ is [] [] for $y \neq x^* $. This means that $f(y)$ [] $f(x^*) \forall y \neq x^* $. Hence $x^*$ is a [] [] of $f$.
-----$f(y) - f(x^*)$ is strictly positive for $y \neq x^* $. This means that $f(y) > f(x^*) \forall y \neq x^* $. Hence $x^*$ is a local minimiser of $f$.


Sufficient conditions of optimality provide a [] of the []. This is because we may have a non-strict minimiser or additional minimisers.
-----Sufficient conditions of optimality provide a subset of the solutions. This is because we may have a non-strict minimiser or additional minimisers.


For convex functions, the [] conditions of optimality are also [].
-----For convex functions, the necessary conditions of optimality are also sufficient.


Continuous $f: R^n \rightarrow R$ is convex iff...
-----$f(y) - f(x) \geq \nabla f(x)^T (y-x)$ holds for all $x$ and $y$


$f(y) - f(x) \geq \nabla f(x)^T (y-x)$ geometric interpretation
-----Curve sits above a "basket" of tangent lines. Epigraph of $f$ is a convex set.


$x^*$ is a global minimiser of convex and continuous $f: R^n \rightarrow R$ iff ...
-----$\nabla f(x^*) = 0$


Generic optimisation algorithm assumptions
-----Given $f: R^n \rightarrow R$ Assume: 1) $f \in C^2$  2) there exists a solution to minimising $f(x)$  3) (the level set for the initial guess) $L(f(x_0))$ is compact [this means there is a global minimiser and this solution is a stationary point]


Optimisation algorithm (most generic definition)
-----An optimisation algorithm is a sequence $\{x_k\}$ obtained from $x_0$ which has some convergence properties with respect to set $\Omega$ (the set of stationary points).


Optimisation algorithm (second most generic definition)
-----1) Given $x_k$  2) Check if $x_k \in \Omega$, and if yes stop  3) Select a direction $d_k$  4) Select a step $\alpha_k$ along $d_k$  5) Pick $x_k+1 = x_k+ \alpha_k d_k$  6) Repeat


Issues with sequence $\{x_k\}$ given $a_k$ and $d_k$:
-----Existence of limit points for $\{x_k\}$, behaviour of the limit points in relation to $\Omega$, speed of convergence to $\Omega$


Bounded Sequence
-----A sequence $X = \{x_n\}$ of real numbers is said to be bounded if there exists a real number $M > 0$ such that $|x_n| \leq M \forall n \in N$.


Which kind of sequence has converging subsequences?
-----Bounded sequences


Descent condition: If $f(x_k+1)$ is always [] than $f(x_k)$ then $\{x_k\}$ belongs to the level set $L(f(x_0))$ which means $||x_k|| \leq M$ which means the sequence is [] and there are [] subsequences.
-----Descent condition: If $f(x_k+1)$ is always smaller than $f(x_k)$ then $\{x_k\}$ belongs to the level set $L(f(x_0))$ which means $||x_k|| \leq M$ which means the sequence is bounded and there are converging subsequences.


Condition of angle
-----A descent direction $d_k$ satisfies a condition of angle if there exists a number $\epsilon > 0$ such that: $\nabla f^T(x_k)d_k \leq -\epsilon ||\nabla f(x_k)|| ||d_k||$


Given the problem of minimising $f(x)$ If: 
$x \in R^n$ and $L(f(x_0))$ is compact, 
The directions $d_k$ are such that $|\text{det}[d_k/||d_k||,...,d_{k+n-1}/||d_{k+n-1}||]| \geq \sigma$ for some $\sigma > 0$, 
$lim_{k \rightarrow \infty}{||x_{k+1} - x_k||} = 0$, 
$f(x_{k+1}) < f(x_k), x \notin \Omega$, 
The property $lim_{k \rightarrow \infty}\nabla f(x_k)^Td_k / ||d_k|| = 0$ holds then...
-----$\{x_k\} \in L(f(x_0))$ and any subsequence of $\{x_k\}$ has limit points, 
$\{f(x_k)\}$ is monotonically non increasing and $lim_{k \rightarrow \infty} f(x) = \bar f$, 
Any limit point of $\{x_k\}$ is such that $\nabla f(x_k) = 0$


We say thet $\{x_k\}$ has speed of convergence of order $p$ if...
-----$\lim_{k \rightarrow \infty} (\epsilon_{k + 1} / \epsilon^p_k) = C_p$ with $p \geq 1$ and $0 < C_p < \infty$


What kind of convergence do efficient minimisation algorithms have?
-----Superlinear convergence


What is a line search?
-----A line search is a method to compute the step $\alpha_k$ along a given direction $d_k$


What conditions does a line search have to impose?
-----$f(x_{k + 1}) < f(x_k)$ and $\lim_{k \rightarrow \infty} \nabla f(x_k)^Td_k / ||d_k|| = 0$ 


What condition should a line search impose whenever possible?
-----$\lim_{k \rightarrow \infty} ||x_{k + 1} - x_k|| = 0$


The exact line search consists in finding $\alpha_k$ such that...
-----$\phi(\alpha_k) \leq \phi(\alpha)$ (f(x_k + \alpha_k d_k) - f(x_k) \leq  f(x_k + \alpha d_k) - f(x_k)) for any $\alpha \geq 0$


Why do we usually not use exact line search?
-----The search for the optimal $\alpha_k$ is computationally expensive and the minimisation algorithm does not necessarily require the optimal $\alpha_k$.


What conditions to approximate line search methods have to satisfy?
-----$\alpha_k$ has to guarantee a sufficient reduction of $f$, $\alpha_k$ has to sufficiently distant from 0 


How to do exact line search analytically
-----Express $f(\mathbf{x}_{k+1})$ in terms of $\alpha_{k+1}$ (given we know $f(\mathbf{x}_{k})$) and then minimise $f(\mathbf{x}_{k+1})$ w.r.t $\alpha_{k+1}$ to find optimal $\alpha_{k+1}$


Set of points $A$ considered in the Armijo method
-----$A = \{  \alpha \in R : \alpha = a \sigma^j , j = 0,1,... \}$ ($\alpha > 0$, $\sigma \in (0,1)$)


What set does the $\gamma$ parameter in the Armijo method belong to?
-----$\gamma \in (0, 1/2)$


Armijo method consists in finding the largest $\alpha \in A$ such that...
-----$f(x_k + \alpha d_k) - f(x_k) \leq \gamma \alpha \nabla f(x_k)^T d_k$


Conceptual algorithm for Armijo method 
-----1) Set $\alpha = a$ 2) if $f(x_k + \alpha d_k) - f(x_k) \leq \gamma \alpha \nabla f(x_k)^T d_k$ set $\alpha = \alpha$ and stop, else 3) Set $\alpha = \sigma \alpha$ and go to 2.


Main disadvantage of Armijo method
-----All points in the set $A$, starting from the point $\alpha = a$, have to be tested until the condition $f(x_k + \alpha d_k) - f(x_k) \leq \gamma \alpha \nabla f(x_k)^T d_k$ is fulfilled.


Goldstein conditions allow us to find an acceptable $\alpha_k$ in [] []
-----Goldstein conditions allow us to find an acceptable $\alpha_k$ in one step


Goldstein conditions state that given $\gamma_1 \in (0, 1)$ and $\gamma_2 \in (0, 1)$ such that $\gamma_1 < \gamma_2$, $\alpha_k$ is any positive number such that...
-----$f(x_k + \alpha d_k) - f(x_k) \leq \alpha_k \gamma_1 \nabla f(x_k)^T d_k$ and $f(x_k + \alpha d_k) - f(x_k) \geq \alpha_k \gamma_2 \nabla f(x_k)^T d_k$


Goldstein conditions geometric interpretation
-----Selecting $\alpha_k$ as any point such that the corresponding value of $f$ is included between 2 straight lines of slope $\gamma_1 \nabla f(x_k)^T d_k$ and $\gamma_2 \nabla f(x_k)^T d_k$ and passing through the point $(0, \phi(0))$.


How can we do a line search if there is no information on the derivatives of the function $f$?
-----There is a modification of Armijo's method called parabolic search that doesn't require derivatives.


What is the gradient method?
-----Select the direction of the anti-gradient at $x_k$ as the research direction, i.e. $d_k = -\nabla f(x_k)$


Newton's method for the minimisation of $f$ can be derived assuming that, given $x_k$, the point $x_{k+1}$ is obtained by... 
-----Minimising a quadratic approximation of $f$. 


Simple Newton's method algorithm for minimising $f$
-----0) Given $x_0 \in R^n$ 1) Set $k = 0$ 2) Compute $s = -[\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$ 3) Set $x_{k+1} = x_k + s, k = k + 1$ and go to step 2


Way to check if quadratic speed of convergence?
-----Convergence quadratic if $\epsilon_{k+1}/\epsilon_k^2 = |x_{k+1} - x^*|/|x_k - x^*|^2 = 1$


Given a matrix $Q = Q^T$, the vectors $d_1$ and $d_2$ are said to by Q-conjugate if...
-----$d^T_1 Q d_2 = 0$